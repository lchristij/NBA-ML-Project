{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "%matplotlib inline\n",
    "\n",
    "# Needed for decision tree visualization\n",
    "import pydotplus\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-communication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "csvpath = Path('../Project-03/teamGameStats.csv')\n",
    "teamstats_df = pd.read_csv(csvpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-producer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping unneeded columns\n",
    "# subsequent analysis dropped '+/-' and 'PTS' columns\n",
    "teamstats_df = teamstats_df.drop(columns=['TEAM','GAMEDATE','+/-','PTS','REB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-links",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting 'H/A' to 0/1's\n",
    "teamstats_df['H/A'] = teamstats_df['H/A'].apply(lambda x: 1 if x==\"vs\" else 0)\n",
    "teamstats_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-terror",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting 'W/L' to 0/1's\n",
    "teamstats_df['W/L']=teamstats_df['W/L'].apply(lambda x: 1 if x==\"W\" else 0)\n",
    "teamstats_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-porcelain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping unneeded columns\n",
    "# subsequent analysis dropped '+/-' and 'PTS' columns\n",
    "teamstats_df = teamstats_df.drop(columns=['TEAM1','TEAM2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_teamstats_df = pd.get_dummies(teamstats_df, columns=['TEAM1','TEAM2']).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-reach",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_teamstats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-suspension",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining 'W/L' column as feature set\n",
    "X = teamstats_df.copy()\n",
    "X.drop('W/L', axis=1, inplace=True)\n",
    "X.head(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-uniform",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "teamstats_df.info('include=all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-grammar",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-baking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining target vector\n",
    "#y=teamstats_df['W/L'].values.reshape(-1,1)\n",
    "# defining target vector\n",
    "y=teamstats_df['W/L'].ravel()\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into Train and Test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-somewhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating StandardScaler instance\n",
    "#scaler = StandardScaler()\n",
    "\n",
    "# Fitting Standard Scaller\n",
    "#X_scaler = scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-absence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling data\n",
    "#X_train_scaled = X_scaler.transform(X_train)\n",
    "#X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-chess",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=30, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-hurricane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "rf_model = rf_model.fit(X_train, y_train)\n",
    "#rf_model = rf_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-kidney",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions using the testing data\n",
    "predictions = rf_model.predict(X_test)\n",
    "#predictions = rf_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "cm_df = pd.DataFrame(\n",
    "    cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    ")\n",
    "\n",
    "# Calculating the accuracy score\n",
    "acc_score = accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-sleep",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Displaying results\n",
    "print(\"Confusion Matrix\")\n",
    "display(cm_df)\n",
    "print(f\"Accuracy Score : {acc_score}\")\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-liberal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forests in sklearn will automatically calculate feature importance\n",
    "importances = rf_model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-welding",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We can sort the features by their importance\n",
    "sorted(zip(rf_model.feature_importances_, X.columns), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Visualize the features by importance\n",
    "importances_df = pd.DataFrame(sorted(zip(rf_model.feature_importances_, X.columns), reverse=True))\n",
    "importances_df.set_index(importances_df[1], inplace=True)\n",
    "importances_df.drop(columns=1, inplace=True)\n",
    "importances_df.rename(columns={0: 'Feature Importances'}, inplace=True)\n",
    "importances_sorted = importances_df.sort_values(by='Feature Importances')\n",
    "importances_sorted.plot(kind='barh', color='lightgreen', title= 'Features Importances', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-bracelet",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
